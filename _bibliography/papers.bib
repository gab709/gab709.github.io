---
---
@article{merlin2023happens,
  title={What Happens During Finetuning of Vision Transformers: An Invariance Based Investigation},
  author={Merlin, Gabriele and Nanda, Vedant and Rawal, Ruchit and Toneva, Mariya},
  journal={arXiv preprint arXiv:2307.06006},
  year={2023},
  publisher = {arXiv},
  selected={true},
  bibtex_show={true},
  abbr={CoLLAs23},
  abstract="The pretrain-finetune paradigm usually improves downstream performance over training a model from scratch on the same task, becoming commonplace across many areas of machine learning. While pretraining is empirically observed to be beneficial for a range of tasks, there is not a clear understanding yet of the reasons for this effect. In this work, we examine the relationship between pretrained vision transformers and the corresponding finetuned versions on several benchmark datasets and tasks. We present new metrics that specifically investigate the degree to which invariances learned by a pretrained model are retained or forgotten during finetuning. Using these metrics, we present a suite of empirical findings, including that pretraining induces transferable invariances in shallow layers and that invariances from deeper pretrained layers are compressed towards shallower layers during finetuning. Together, these findings contribute to understanding some of the reasons for the successes of pretrained models and the changes that a pretrained model undergoes when finetuned on a downstream task.",
  html={https://arxiv.org/abs/2212.00596},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{merlinlanguage,
  doi = {10.48550/ARXIV.2212.00596},
  
  url = {https://arxiv.org/abs/2212.00596},
  
  author = {Merlin, Gabriele and Toneva, Mariya},
  
  keywords = {Computation and Language (cs.CL), Neurons and Cognition (q-bio.NC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Biological sciences, FOS: Biological sciences},
  
  title = {Language models and brain alignment: beyond word-level semantics and prediction},
  abstract="Pretrained language models that have been trained to predict the next word over billions of text documents have been shown to also significantly predict brain recordings of people comprehending language. Understanding the reasons behind the observed similarities between language in machines and language in the brain can lead to more insight into both systems. Recent works suggest that the prediction of the next word is a key mechanism that contributes to the alignment between the two. What is not yet understood is whether prediction of the next word is necessary for this observed alignment or simply sufficient, and whether there are other shared mechanisms or information that is similarly important. In this work, we take a first step towards a better understanding via two simple perturbations in a popular pretrained language model. The first perturbation is to improve the model's ability to predict the next word in the specific naturalistic stimulus text that the brain recordings correspond to. We show that this indeed improves the alignment with the brain recordings. However, this improved alignment may also be due to any improved word-level or multi-word level semantics for the specific world that is described by the stimulus narrative. We aim to disentangle the contribution of next word prediction and semantic knowledge via our second perturbation: scrambling the word order at inference time, which reduces the ability to predict the next word, but maintains any newly learned word-level semantics. By comparing the alignment with brain recordings of these differently perturbed models, we show that improvements in alignment with brain recordings are due to more than improvements in next word prediction and word-level semantics.",
  publisher = {arXiv},
  selected={true},
  year = {2022},
  bibtex_show={true},
  abbr={Preprint},
  html={https://arxiv.org/abs/2212.00596},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@InProceedings{merlinpractical,
author="Merlin, Gabriele
and Lomonaco, Vincenzo
and Cossu, Andrea
and Carta, Antonio
and Bacciu, Davide",
editor="Mazzeo, Pier Luigi
and Frontoni, Emanuele
and Sclaroff, Stan
and Distante, Cosimo",
title="Practical Recommendations forÂ Replay-Based Continual Learning Methods",
booktitle="Image Analysis and Processing. ICIAP 2022 Workshops",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="548--559",
selected={true},
abstract="Continual Learning requires the model to learn from a stream of dynamic, non-stationary data without forgetting previous knowledge. Several approaches have been developed in the literature to tackle the Continual Learning challenge. Among them, Replay approaches have empirically proved to be the most effective ones [16]. Replay operates by saving some samples in memory which are then used to rehearse knowledge during training in subsequent tasks. However, an extensive comparison and deeper understanding of different replay implementation subtleties is still missing in the literature. The aim of this work is to compare and analyze existing replay-based strategies and provide practical recommendations on developing efficient, effective and generally applicable replay-based strategies. In particular, we investigate the role of the memory size value, different weighting policies and discuss about the impact of data augmentation, which allows reaching better performance with lower memory sizes.",
isbn="978-3-031-13324-4",
bibtex_show={true},
abbr={ICIAP22},
html={https://link.springer.com/chapter/10.1007/978-3-031-13324-4_47}
}

@inproceedings {merlindesign,
booktitle = {Smart Tools and Apps for Graphics - Eurographics Italian Chapter Conference},
editor = {Agus, Marco and Corsini, Massimiliano and Pintus, Ruggero},
title = {{Design and Implementation of a Visualization Tool for the in-depth Analysis of the Domestic Electricity Consumption}},
author = {Merlin, Gabriele and Ortu, Daniele and Cherchi, Gianmarco and Scateni, Riccardo},
year = {2019},
publisher = {The Eurographics Association},
ISSN = {2617-4855},
ISBN = {978-3-03868-100-7},
DOI = {10.2312/stag.20191368},
bibtex_show={true},
selected={true},
abbr={STAG19},
html={https://diglib.eg.org/handle/10.2312/stag20191368},
abstract="In this poster, we present a visualization tool for the in-depth analysis of domestic electricity consumption. The web-interface allows users to visualize their electricity consumption, compare them with their own records or with the means of selected communities",
}

